{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Data Cleaning and Preprocessing\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "#from sqlalchemy import create_engine\n",
    "#from config import db_password\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "# Read in files\n",
    "df = pd.read_csv('Resources/bitcoin_data.zip')\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "\n",
    "# Display initial data\n",
    "df.head(10)\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "# Drop all nulls\n",
    "# df = df.dropna()\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "\n",
    "# keep timestamp, high, low, weighted_price\n",
    "df = df[['Timestamp', 'High', 'Low', 'Volume_(BTC)', 'Weighted_Price']].reset_index(drop=True)\n",
    "df\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "\n",
    "# preprocessing notes-- consider omitting data prior to Jan 1st, 2012 due to higher volatility \n",
    "# earlier in bitcoin's lifecycle\n",
    "\n",
    "# converting Timestamp column to str datatype\n",
    "df['str_timestamp'] = [str(timestamp) for timestamp in df['Timestamp']]\n",
    "df['int_timestamp'] = df['str_timestamp'].astype('int32', copy=True)\n",
    "\n",
    "# filtering df to only include rows after 01/01/2012\n",
    "df_cleaned_filtered = df.loc[df['int_timestamp'] >= 1325391360]\n",
    "df_cleaned_filtered.head(2)\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "\n",
    "# context manager to suppress 1 time SettingWithCopyWarning; alternatively call .loc after timestamp conversion to avoid error\n",
    "pd.reset_option('mode.chained_assignment')\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    df_cleaned_filtered['UTC_time'] = df_cleaned_filtered['int_timestamp'].apply(lambda x: datetime.datetime.utcfromtimestamp(x))\n",
    "\n",
    "# .strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "# Convert from Unix to UTC time\n",
    "#df_cleaned_filtered['UTC_time'] = df_cleaned_filtered['int_timestamp'].\\\n",
    "#    apply(lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "\n",
    "# dropping all null values, followed by evaluation of which time periods had the greatest data loss\n",
    "final_df = df_cleaned_filtered.dropna().reset_index(drop=True)\n",
    "\n",
    "# finding percentage of data loss (null values)\n",
    "btotalLength = (len(df_cleaned_filtered))\n",
    "bpartialLength = (len(final_df))\n",
    "bValuesDropped = btotalLength - bpartialLength\n",
    "bitstampDiff = (btotalLength - bpartialLength) / btotalLength * 100 \n",
    "totalDays = round(btotalLength/1440,2)\n",
    "daysDropped = round(bValuesDropped/1440,2)\n",
    "\n",
    "print(f\"Percentage of total Bitstamp data lost due to dropping rows with NaN values: {bitstampDiff:.2f}%\")\n",
    "print(f\"Ratio of Bitstamp rows dropped: {bValuesDropped:,}/{btotalLength:,}\")\n",
    "print(f\"Total dataset contains {totalDays} days worth of data. Due to data loss, we had to drop {daysDropped} days worth of data\")\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "# calculating UTC time deltas\n",
    "final_df['time_delta'] = (final_df['UTC_time'] - final_df['UTC_time'].shift())\n",
    "\n",
    "# creating mask to filter deltas\n",
    "mask = final_df['time_delta'].apply(lambda x: True if x <= pd.Timedelta('0 days 00:01:00') else False)\n",
    "\n",
    "# invert mask to create df with significant data loss\n",
    "deltas_df = final_df[~mask]\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "# shows longest periods of data loss\n",
    "sorted_deltas_df = deltas_df.sort_values('time_delta', ascending=False, ignore_index=False)\n",
    "sorted_deltas_df\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "# shows periods of data loss where weighted price was highest\n",
    "sorted_prices_df = deltas_df.sort_values('Weighted_Price', ascending=False, ignore_index=False)\n",
    "sorted_prices_df\n",
    "\n",
    "\n",
    "# In[55]:\n",
    "\n",
    "\n",
    "# Machine learning section\n",
    "# possible implementation ideas:\n",
    "# 1. time series forecasting bitcoin price with ARIMA(Auto Regressive Integrated Moving Average)\n",
    "# Rational for using time series: the 4 components of time series are trends, seasonality (consistent trends that span 1 calender year), noise/irregularity, and cyclicity(trends that can be observed in <1 or >1 year); this is perfect for tradeable assets \n",
    "# Qualifying criteria: the dataset must be stationary in order to more accurately fit a time series model. Stationary means that there should be a constant mean with constant std deviation or variance\n",
    "# We can test if a time series is stationary by plotting the moving avg and/or movning std dev. to see if it changes over time. These results can be supported with an ADCF test\n",
    "# if testing reveals our time series is NOT stationary, we can perform a variety of transformations to change it (log transform, sq rt, exponential decay etc..)\n",
    "# SARIMA is ARIMA with a seasonality component. \n",
    "\n",
    "\n",
    "# ### PGAdmin SQL Schema\n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "# db schema thoughts: can create multiple tables for each currency (1 for btc, 1 ethereum, 1 xrp)\n",
    "# can also add in additional tables down the line to support our analysis (table with google analytics, table with twitter search results for sentiment analysis)\n",
    "\n",
    "\n",
    "# In[57]:\n",
    "\n",
    "\n",
    "# View final dataset column names\n",
    "final_df.head(5)\n",
    "\n",
    "\n",
    "# In[58]:\n",
    "\n",
    "\n",
    "# Rename columns to be sql friendly\n",
    "final_df.rename({'Timestamp':'timestamp',\n",
    "                  'High':'high',\n",
    "                  'Low':'low',\n",
    "                  'Volume_(BTC)':'volume',\n",
    "                  'Weighted_Price':'weightedPrice',\n",
    "                  'str_timestamp':'timestampSTR',\n",
    "                  'int_timestamp':'timestampINT',\n",
    "                  'UTC_time':'timeUTC',\n",
    "                  'time_delta':'timeDelta'\n",
    "                 }, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "# Converting timedelta to string\n",
    "final_df = final_df.astype({'timeDelta':'str'})\n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "# Checking datatypes\n",
    "final_df.dtypes\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "# Verifying final dataframe\n",
    "final_df.head(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
